# -*- coding: utf-8 -*-
"""Spark_6580081.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eV1sxZpWn6APGNHDKB43oQhyAchubnV7

1. Import a text file of your choice
2. Do a word count..
3. Remove special characters from the word count
4. Sort by key
5. Save the result
6. Run on Google cluster/ Docker
"""

# Installing required packages
#pip install pyspark
#pip install findspark

#import findspark
#findspark.init()

from pyspark import SparkContext,SparkConf
# Creating a spark context class
sc = SparkContext()

"""Import textfile and create an RDD command"""

raw = sc.textFile("gs://sparkcovid/covid.txt")

raw.getNumPartitions()

print(raw.glom().collect())

word_RDD = raw.flatMap(lambda line : line.split(" "))
print(word_RDD.glom().collect())

"""WordCount"""

word_RDD_map = word_RDD.map(lambda word : (word,1))
sum = word_RDD_map.reduceByKey(lambda a, b:(a + b))
for lines in sum.collect():
  print(lines)

"""Remove special chars from WordCount"""

import re
removed_special_chars = sum.map(lambda pair:(re.sub(r'[^a-zA-Z0-9]', '', pair[0]),pair[1]))
filtered = removed_special_chars.filter(lambda pair: pair[0] != '') #filter empty space

for lines in filtered.collect():
  print(lines)

"""Sort by key"""

sorted = filtered.sortByKey()
for lines in sorted.collect():
  print(lines)

"""Save result"""

sorted.saveAsTextFile("gs://sparkcovid/output")